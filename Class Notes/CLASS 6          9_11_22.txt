CLASS 6          9/11/22


DECISION TREE, KNN, RANDOM FOREST


DECISION TREE- leaf nodes are y values
Entropy, decision tree classification


from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(x,y)


#Predicting new result
regressor.predict([[6.5]])


Graph looks like step func


Correlation between no of columns and height of tree
Correlation between no of rows and width/children of the tree/node


  

Columns- country, salary
Rows- italy, germany under country




Training The decision tree classification model on the training set
from sklearn.tree import DecisionTreeClassifier
classifier= DecisionTreeClassifier(criterion=’entropy’ , random_state=0)
classifier.fit(x_train,y_train)


print(classifier.predict(sc.transform([[30,87000]]))
[0]




#making the confusion matrix
from sklearn.metrics import confusion_matrix, accuray_score
cm = confusion_matrix(y_test,y_pred)
print(cm)
accuracy_score(y_test,y_pred)
[[62 6]
 [ 3 29]]
0.91
91% accuracy


Heatmap – range of data
When there are more clusters - more accurate
When giving more emphasis to range of values




KNN : K NEAREST NEIGHBOURS
Mainly used as a classifier


1)despite being closer, whoever are looking in a convenient place, become partners
2)new point is placed in a conflicting position 50/50, 
Choosing partner depends on whoever quicker
3) convenient places form convenient sets
4) too many too close- can cause confusion in the formation of sets of a ceratin number


The certain  number(k-value) must be appropriate
Outliers being present is a problem as they may not be included. And any close points left out may try to group up with outliers which is a problem.
Placed smack dab in the middle is in a conflicting position


Groups formed based on graph


inference - you have similar attributes to the points around you
Ie the classification of the majority around you is what the point will adopt as its own
If numerical- average


Very simple, less training time, time taken to give output is high as distance needs to be calculated(euclidean distance/manhattan) between k nearest neighbours each time


Training knn model on training set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier




P- starting point




Here knn gives accuracy of 0.93 higher than decision


Heatmap works best where there is classifier, anywhere we need to give more importance to certain ranges/categories


No working in training, but more work in predicting










RANDOM FOREST
Random collection of trees


data checks each tree to find which gives most accurate classification
It gives weights to different models ex- knn, svm, decision trees, polynomial regression
It performs all and gives weights to each and then finds the most probable result




Random forest - accuracy rating– 0.91